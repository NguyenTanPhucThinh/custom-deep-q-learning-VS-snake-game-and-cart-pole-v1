{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d6c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import snake_env\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "99749f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba3c2c",
   "metadata": {},
   "source": [
    "In addition, we also have to initialise our hyper parameters\n",
    "MEMORY_SIZE determines how large the buffer is, used to store experiences\n",
    "GAMMA is a discount where the future reward have a tendency to be decreased overtime\n",
    "ALPHA is the learning rate we used for the update in deep Q learning\n",
    "NUM_STEPS_FOR_UPDATE determines how many steps we have to acquire before updating the q network and soft updating the target q network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2f36b",
   "metadata": {},
   "source": [
    "import all necessary library to build ourself deep q learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853801ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = snake_env.SnakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708b77e",
   "metadata": {},
   "source": [
    "We set up some display so the user can see inside the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e58df3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.], dtype=float32),\n",
       " {'score': 0})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ec7b8",
   "metadata": {},
   "source": [
    "but first, we have to reset our environment to the initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01c9471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (11,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0a42ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      "0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, _ = env.step(action = 3)\n",
    "print(observation)\n",
    "print(reward)\n",
    "print(terminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d97e8a",
   "metadata": {},
   "source": [
    "    ACTION:\n",
    "    -UP = 0\n",
    "    -RIGHT = 1\n",
    "    -DOWN = 2\n",
    "    -LEFT = 3\n",
    "    OBSERVATION\n",
    "    -danger_straight\n",
    "    -danger_right\n",
    "    -danger_left\n",
    "    -dir_up\n",
    "    -dir_right\n",
    "    -dir_down\n",
    "    -dir_left,\n",
    "    -food_up\n",
    "    -food_down\n",
    "    -food_left\n",
    "    -food_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "57d4582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape = state_size),\n",
    "    tf.keras.layers.Dense(units = 64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(units = 64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(units = num_actions, activation = 'linear')\n",
    "    ])\n",
    "target_q_network = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape = state_size),\n",
    "    tf.keras.layers.Dense(units = 64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(units = 64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(units = num_actions, activation = 'linear')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505aa58",
   "metadata": {},
   "source": [
    "Next we will initialize 2 networks, q network and target q network, both share the same architecture. We can only just initialize 1 network only, however, for alot of reasons: such as for stable learning, avoiding oscillations, etc..\n",
    "We will use 2 networks which will help our agent stable the learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fcb1b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate= ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f7498",
   "metadata": {},
   "source": [
    "Initialize our Adam optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
